{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce74131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biodatasets import list_datasets, load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be605d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name=pathogen, available_columns=['sequence', 'sequence_id', 'class'], available_embeddings={('sequence', 'protbert', 'cls')})\n",
      "# \"Human vs pathogen\" dataset\n",
      "\n",
      "\n",
      "## Dataset Description\n",
      "Human vs pathogen clasification dataset.\n",
      "\n",
      "### Dataset Summary\n",
      "\n",
      "Human vs pathogen clasification dataset. 96k protein sequences (50% human, 50% pathogens). Extracted from Uniprot. Embeddings available calculated with ProtBert\n",
      "\n",
      "Features:\n",
      " - sequence\n",
      " - sequence_id\n",
      "\n",
      "Embeddings:\n",
      " - CLS embeddings - 1024-dim\n",
      "\n",
      "Label:\n",
      " - class\n",
      "  - 0: human\n",
      "  - 1: pathogen\n",
      "\n",
      "### Usage\n",
      "```\n",
      "from biodatasets import load_dataset\n",
      "\n",
      "pathogen_dataset = load_dataset(\"pathogen\")\n",
      "\n",
      "X, y = pathogen_dataset.to_npy_array(input_names=[\"sequence\"], target_names=[\"class\"])\n",
      "cls_embeddings = pathogen_dataset.get_embeddings(\"sequence\", \"protbert\", \"cls\")\n",
      "```\n",
      "\n",
      "### Supported Tasks\n",
      " - clasification\n",
      " - inmunogenecity\n",
      "\n",
      "### Model used to calculate Embeddings\n",
      " - ProtBert\n",
      "\n",
      "### Libraries used to calculate embeddings\n",
      " - Pytorch\n",
      "\n",
      "\n",
      "### Source Data\n",
      "\n",
      "[Uniprot](https://www.uniprot.org/)\n",
      "\n",
      "\n",
      "### Dataset Curators\n",
      "\n",
      "[DeepChain team](https://deepchain.bio)\n",
      "\n",
      "### Licensing Information\n",
      "[Creative Commons Attribution (CC BY 4.0)](https://www.uniprot.org/help/license) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading data into numpy array\n",
    "pathogen = load_dataset(\"pathogen\")\n",
    "\n",
    "X, y = pathogen.to_npy_arrays(input_names=[\"sequence\"], target_names=[\"class\"])\n",
    "\n",
    "pathogen.display_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1317c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T': 0, 'Y': 1, 'M': 2, 'L': 3, 'K': 4, 'H': 5, 'Z': 6, 'R': 7, 'A': 8, 'P': 9, 'F': 10, 'C': 11, 'V': 12, 'I': 13, 'S': 14, 'U': 15, 'Q': 16, 'D': 17, 'N': 18, 'B': 19, 'E': 20, 'G': 21, 'X': 22, 'W': 23}\n"
     ]
    }
   ],
   "source": [
    "# Encoding Amino Acids to number\n",
    "def get_seq_column_map(X):\n",
    "    unique = set()\n",
    "    for idx, sequence in enumerate(X[0]):\n",
    "        unique.update(list(sequence))\n",
    "    \n",
    "    return dict(zip(unique, list(range(len(unique)))))\n",
    "    \n",
    "pathogen_map = get_seq_column_map(X)\n",
    "print(pathogen_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7869b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.,  4., 17.,  ...,  0.,  0.,  0.],\n",
       "         [ 2., 13., 18.,  ...,  0.,  0.,  0.],\n",
       "         [ 2.,  3., 12.,  ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 2.,  2.,  3.,  ...,  0.,  0.,  0.],\n",
       "         [17., 10., 12.,  ...,  0.,  0.,  0.],\n",
       "         [ 2., 20.,  8.,  ..., 20.,  4.,  9.]]),\n",
       " tensor([[0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PathogenDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pathogen_map, data):\n",
    "        self.pathogen_map = pathogen_map\n",
    "        self.X = data[0]\n",
    "        self.Y = self.__one_hot(data[1])\n",
    "        \n",
    "    def __one_hot(self, Y):\n",
    "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "        one_hot_Y = one_hot_Y\n",
    "        return one_hot_Y.astype(np.float64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.as_tensor([self.pathogen_map[e] for e in list(self.X[idx])]) \n",
    "        Y = self.Y[idx]\n",
    "        return X, Y\n",
    "\n",
    "def collate_padd(batch):\n",
    "        x = [row[0] for row in batch]\n",
    "        y = [row[1] for row in batch]\n",
    "        x =  pad_sequence(x, batch_first=True)\n",
    "        return torch.as_tensor(x).to(torch.float32), torch.as_tensor(y).to(torch.float32)\n",
    "    \n",
    "# Split ~ 80% 10% 10%\n",
    "training_set = PathogenDataset(pathogen_map,(X[0][:80000], y[0][:80000]))\n",
    "training_loader = DataLoader(training_set, batch_size=8, shuffle=True, collate_fn=collate_padd)\n",
    "\n",
    "validation_set = PathogenDataset(pathogen_map,(X[0][80000:90000], y[0][80000:90000]))\n",
    "validation_loader = DataLoader(validation_set, batch_size=8, collate_fn=collate_padd)\n",
    "\n",
    "testing_set = PathogenDataset(pathogen_map,(X[0][90000:], y[0][90000:]))\n",
    "testing_loader = DataLoader(testing_set, batch_size=8, collate_fn=collate_padd)\n",
    "\n",
    "next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b091894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=len(pathogen_map)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=input_dim,\n",
    "            embedding_dim=512,\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.linear_1 = nn.Linear(\n",
    "            in_features=256,\n",
    "            out_features=128,\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.linear_2 = nn.Linear(\n",
    "            in_features=128,\n",
    "            out_features=2,\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        embed = self.embed(x)\n",
    "        \n",
    "        lstm_1_seq, (lstm_1_h, lstm1_c) = self.lstm(embed)\n",
    "        \n",
    "        linear_1 = self.linear_1(torch.squeeze(lstm_1_h))\n",
    "        dropout = self.dropout(linear_1)\n",
    "        linear_2 = self.linear_2(dropout)\n",
    "        \n",
    "        softmax = self.softmax(linear_2)\n",
    "        \n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550796d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2060 SUPER'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device : {device}\")\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32b5544e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embed): Embedding(24, 512)\n",
      "  (lstm): LSTM(512, 256, batch_first=True)\n",
      "  (linear_1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (linear_2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0: 100%|████████████████████████████| 10000/10000 [06:15<00:00, 26.65it/s]\n",
      "epoch 1: 100%|████████████████████████████| 10000/10000 [05:39<00:00, 29.43it/s]\n",
      "epoch 2: 100%|████████████████████████████| 10000/10000 [05:36<00:00, 29.74it/s]\n",
      "epoch 3: 100%|████████████████████████████| 10000/10000 [05:36<00:00, 29.71it/s]\n",
      "epoch 4: 100%|████████████████████████████| 10000/10000 [05:36<00:00, 29.71it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Net().cuda()\n",
    "print(model)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    tqdm_bar = tqdm(training_loader, desc=f\"epoch {epoch}\", position=0)\n",
    "    \n",
    "    # Training\n",
    "    for idx, (inputs, labels) in enumerate(tqdm_bar):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs.to(torch.int32))\n",
    "        \n",
    "        loss = criterion(outputs, labels).to(torch.float32)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        writer.add_scalar('Loss/train', loss, idx)\n",
    "        \n",
    "        # Training Accuracy\n",
    "        correct, total = 0, 0\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, y = torch.max(labels, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        writer.add_scalar('accuracy/train', correct/total, idx)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validation Accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(validation_loader):\n",
    "            correct, total = 0, 0\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = model(inputs.to(torch.int32))\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, y = torch.max(labels, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            writer.add_scalar('accuracy/validation', correct/total, idx)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fe30cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-18 13:42:04.056767: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:04.056787: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-18 13:42:05.143385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 13:42:05.143653: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:05.143687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:05.143719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:05.143751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:05.143779: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:05.143808: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:05.143837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:05.143864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-18 13:42:05.143871: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.9.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f9cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './pathogen_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "632421db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of nn: 0.8643934632306726\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_dim).cuda()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Testing Accuracy\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for (inputs, labels) in testing_loader:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        outputs = model(inputs.to(torch.int32))\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, y = torch.max(labels, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "print(f'Accuracy of nn: {correct / total}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
