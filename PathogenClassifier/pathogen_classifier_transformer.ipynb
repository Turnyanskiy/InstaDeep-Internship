{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f322de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biodatasets import list_datasets, load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into numpy array\n",
    "pathogen = load_dataset(\"pathogen\")\n",
    "\n",
    "X, y = pathogen.to_npy_arrays(input_names=[\"sequence\"], target_names=[\"class\"])\n",
    "\n",
    "pathogen.display_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00226a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Amino Acids to number\n",
    "def get_seq_column_map(X):\n",
    "    unique = set()\n",
    "    for idx, sequence in enumerate(X[0]):\n",
    "        unique.update(list(sequence))\n",
    "    \n",
    "    return dict(zip(unique, list(range(len(unique)))))\n",
    "    \n",
    "pathogen_map = get_seq_column_map(X)\n",
    "print(pathogen_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathogenDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pathogen_map, data):\n",
    "        self.pathogen_map = pathogen_map\n",
    "        self.X = data[0]\n",
    "        self.Y = data[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.as_tensor([self.pathogen_map[e] for e in list(self.X[idx])]) \n",
    "        Y = self.Y[idx]\n",
    "        return X, Y\n",
    "\n",
    "def collate_padd(batch):\n",
    "        x = [row[0] for row in batch]\n",
    "        y = [row[1] for row in batch]\n",
    "        \n",
    "        sequence_len = [len(row) for row in x]\n",
    "        x =  pad_sequence(x, batch_first=True)\n",
    "        return (torch.as_tensor(x).to(torch.float32), torch.as_tensor(sequence_len)), torch.as_tensor(y).to(torch.float32)\n",
    "    \n",
    "# Split ~ 80% 10% 10%\n",
    "training_set = PathogenDataset(pathogen_map,(X[0][:80000], y[0][:80000]))\n",
    "training_loader = DataLoader(training_set, batch_size=4, shuffle=True, collate_fn=collate_padd)\n",
    "\n",
    "validation_set = PathogenDataset(pathogen_map,(X[0][80000:90000], y[0][80000:90000]))\n",
    "validation_loader = DataLoader(validation_set, batch_size=8, collate_fn=collate_padd)\n",
    "\n",
    "testing_set = PathogenDataset(pathogen_map,(X[0][90000:], y[0][90000:]))\n",
    "testing_loader = DataLoader(testing_set, batch_size=8, collate_fn=collate_padd)\n",
    "\n",
    "next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbf6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4a4cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        nhead=8,\n",
    "        dim_feedforward=512,\n",
    "        num_layers=6,\n",
    "        activation=\"relu\",\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #vocab_size, d_model = embeddings.size()\n",
    "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
    "\n",
    "        #self.emb = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
    "\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=512,\n",
    "        )\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            max_len=11000,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, 1)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device : {device}\")\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ef9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(\n",
    "    vocab_size=len(pathogen_map),\n",
    "    d_model=512,\n",
    "    nhead=8,  \n",
    "    dim_feedforward=50,\n",
    "    num_layers=6,\n",
    "    dropout=0.25\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(2):\n",
    "    tqdm_bar = tqdm(training_loader, desc=f\"epoch {epoch}\", position=0)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for idx, ((inputs, sequence_len), labels) in enumerate(tqdm_bar):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        outputs = sigmoid(model(inputs.to(torch.int32)))\n",
    "        \n",
    "        print(outputs.flatten(), labels)\n",
    "        \n",
    "        loss = criterion(outputs.flatten(), labels).to(torch.float32)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        writer.add_scalar('Loss/train', loss, idx)\n",
    "        \n",
    "    # Training Accuracy\n",
    "    correct, total = 0, 0\n",
    "    predicted = torch.round(outputs.flatten())\n",
    "    y = labels\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == y).sum().item()\n",
    "    writer.add_scalar('accuracy/train', correct/total, idx)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    \"\"\"\n",
    "    # Validation Accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(validation_loader):\n",
    "            correct, total = 0, 0\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = model(inputs.to(torch.int32))\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, y = torch.max(labels, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            writer.add_scalar('accuracy/validation', correct/total, idx)\n",
    "    \"\"\"\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './pathogen_net_transformer.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(\n",
    "    vocab_size=len(pathogen_map),\n",
    "    d_model=512,\n",
    "    nhead=8,  \n",
    "    dim_feedforward=50,\n",
    "    num_layers=6,\n",
    "    dropout=0.25\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Testing Accuracy\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    all_predicted, all_y = [], []\n",
    "    for ((inputs, sequence_len), labels) in testing_loader:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        outputs = sigmoid(model(inputs.to(torch.int32)))\n",
    "        \n",
    "        predicted = torch.round(outputs.flatten())\n",
    "        y = labels\n",
    "                \n",
    "        all_predicted.extend(predicted.tolist())\n",
    "        all_y.extend(y.tolist())\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "print(confusion_matrix(all_y, all_predicted))\n",
    "print(f'Accuracy of nn: {correct / total}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
